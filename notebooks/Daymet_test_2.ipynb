{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works through a set of tests to calculate the weights daymet contributing cells to each hru in GFv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import zipfile\n",
    "import rasterio\n",
    "import os\n",
    "import xarray as xr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:\\GitRepos\\onhm-fetcher-parser\\notebooks\n",
      "..\\Data\n",
      "       POI_ID  hru_id_nat  hru_id_reg region  \\\n",
      "0     7733855           1           1     01   \n",
      "1     7733919           2           2     01   \n",
      "2     7732571           3           3     01   \n",
      "3     7732387           4           4     01   \n",
      "4     7733327           5           5     01   \n",
      "...       ...         ...         ...    ...   \n",
      "2457   719280        2458        2458     01   \n",
      "2458   805345        2459        2459     01   \n",
      "2459   806793        2460        2460     01   \n",
      "2460   807073        2461        2461     01   \n",
      "2461   725382        2462        2462     01   \n",
      "\n",
      "                                               geometry  \n",
      "0     POLYGON ((-73.37148 41.11233, -73.37151 41.112...  \n",
      "1     POLYGON ((-73.38532 41.13467, -73.38566 41.134...  \n",
      "2     POLYGON ((-73.41947 41.16068, -73.41955 41.160...  \n",
      "3     POLYGON ((-73.41097 41.15833, -73.41105 41.158...  \n",
      "4     MULTIPOLYGON (((-73.28636 41.12781, -73.28645 ...  \n",
      "...                                                 ...  \n",
      "2457  POLYGON ((-69.06651 46.62038, -69.06662 46.620...  \n",
      "2458  MULTIPOLYGON (((-68.81500 46.67176, -68.81493 ...  \n",
      "2459  MULTIPOLYGON (((-69.09013 46.50983, -69.09024 ...  \n",
      "2460  POLYGON ((-68.97849 46.47029, -68.97886 46.470...  \n",
      "2461  POLYGON ((-68.69319 46.93276, -68.69314 46.932...  \n",
      "\n",
      "[2462 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "from pathlib import Path\n",
    "folder = Path(r'../Data') # assumes working directory is onhm-fetcher-parser\n",
    "print(folder)\n",
    "shapefiles = folder.glob(\"*_0[1].shp\")\n",
    "#shapefiles = folder.glob(\"*.shp\")\n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "gdf.reset_index(drop=True, inplace=True)\n",
    "# gdf.plot()\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below (used when all hru shapefiles are read) is not optimum because it's base on the centroid of the hrus.  Therefore these boundary values are better and cover hrus outside the conus -126 54 -65 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-73.73815293899997 48.100042571000074 -66.94953853699997 40.98519585200006\n"
     ]
    }
   ],
   "source": [
    "type(gdf['geometry'].bounds['minx'])\n",
    "west = gdf['geometry'].bounds['minx'].min()\n",
    "east = gdf['geometry'].bounds['maxx'].max()\n",
    "south = gdf['geometry'].bounds['miny'].min()\n",
    "north = gdf['geometry'].bounds['maxy'].max()\n",
    "print(west, north, east, south)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some example URLs for for daymet\n",
    " \n",
    "https://thredds.daac.ornl.gov/thredds/ncss/grid/daymet-v3-agg/na.ncml/dataset.html\n",
    "\n",
    "https://thredds.daac.ornl.gov/thredds/ncss/daymet-v3-agg/na.ncml?var=lat&var=lon&var=dayl&var=prcp&var=srad&var=swe&var=tmax&var=tmin&var=vp&disableLLSubset=on&disableProjSubset=on&horizStride=1&time_start=2019-09-22T12%3A00%3A00Z&time_end=2018-12-31T12%3A00%3A00Z&timeStride=1&accept=netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from requests.exceptions import HTTPError\n",
    "# from datetime import datetime, timedelta\n",
    "# from urllib.parse import urlencode\n",
    "\n",
    "# prcpurl = 'https://thredds.daac.ornl.gov/thredds/ncss/daymet-v3-agg/na.ncml'\n",
    "# prcppayload = {\n",
    "# #     'var': 'lat&var=lon&var=tmax',\n",
    "#     'var': 'lat&var=lon&var=prcp&var=srad&var=swe&var=tmax&var=tmin&var=vp',\n",
    "#     'north': '54',\n",
    "#     'west': '-126',\n",
    "#     'east': '-65',\n",
    "#     'south': '20',\n",
    "#     'disableProjSubset': 'on',\n",
    "#     'horizStride': '1',\n",
    "#     'time_start': '2018-12-31T12:00:00Z',\n",
    "#     'time_end': '2018-12-31T12:00:00Z',\n",
    "#     'timeStride': '1',\n",
    "#     'accept': 'netcdf'}    \n",
    "# try:\n",
    "#     s = requests.Session()\n",
    "#     #https://github.com/psf/requests/issues/1454\n",
    "#     qry = urlencode(prcppayload).replace('%26','&')\n",
    "#     qry = qry.replace('%3D', '=')\n",
    "#     print(qry)\n",
    "#     tmaxfile = requests.get(prcpurl, params=qry)\n",
    "#     tmaxfile.raise_for_status()\n",
    "# except HTTPError as http_err:\n",
    "#     print(f'HTTP error occured: {http_err}')\n",
    "# except Exception as err:\n",
    "#     print(f'Other error occured: {err}')\n",
    "# else:\n",
    "#     print('Gridmet data retrieved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tmax_test2.nc', 'wb') as fh:\n",
    "#     fh.write(tmaxfile.content)\n",
    "# fh.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax_test2.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:                  (time: 1, x: 6008, y: 3699)\n",
      "Coordinates:\n",
      "  * y                        (y) float32 1687.0 1686.0 ... -2010.0 -2011.0\n",
      "  * x                        (x) float32 -2754.25 -2753.25 ... 3251.75 3252.75\n",
      "  * time                     (time) datetime64[ns] 2018-12-31\n",
      "Data variables:\n",
      "    lat                      (y, x) float32 ...\n",
      "    lambert_conformal_conic  int16 ...\n",
      "    lon                      (y, x) float32 ...\n",
      "    prcp                     (time, y, x) float32 ...\n",
      "    srad                     (time, y, x) float32 ...\n",
      "    swe                      (time, y, x) float32 ...\n",
      "    tmax                     (time, y, x) float32 ...\n",
      "    tmin                     (time, y, x) float32 ...\n",
      "    vp                       (time, y, x) float32 ...\n",
      "Attributes:\n",
      "    _NCProperties:       version=1|netcdflibversion=4.4.1|hdf5libversion=1.8.17\n",
      "    start_year:          1980\n",
      "    source:              Daymet Software Version 3.0\n",
      "    Version_software:    Daymet Software Version 3.0\n",
      "    Version_data:        Daymet Data Version 3.0\n",
      "    Conventions:         CF-1.6\n",
      "    citation:            Please see http://daymet.ornl.gov/ for current Dayme...\n",
      "    references:          Please see http://daymet.ornl.gov/ for current infor...\n",
      "    title:               Daymet: Daily Surface Weather Data on a 1-km Grid fo...\n",
      "    institution:         Oak Ridge National Laboratory Distributed Active Arc...\n",
      "    end_year:            2018\n",
      "    History:             Translated to CF-1.0 Conventions by Netcdf-Java CDM ...\n",
      "    geospatial_lat_min:  18.552629168112905\n",
      "    geospatial_lat_max:  58.1720239604238\n",
      "    geospatial_lon_min:  -143.08286944629194\n",
      "    geospatial_lon_max:  -50.72062839852924\n",
      "\n",
      " The meta data is: \n",
      " {'_NCProperties': 'version=1|netcdflibversion=4.4.1|hdf5libversion=1.8.17', 'start_year': 1980, 'source': 'Daymet Software Version 3.0', 'Version_software': 'Daymet Software Version 3.0', 'Version_data': 'Daymet Data Version 3.0', 'Conventions': 'CF-1.6', 'citation': 'Please see http://daymet.ornl.gov/ for current Daymet data citation information', 'references': 'Please see http://daymet.ornl.gov/ for current information on Daymet references', 'title': 'Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 3 (Continental North America)', 'institution': 'Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)', 'end_year': 2018, 'History': 'Translated to CF-1.0 Conventions by Netcdf-Java CDM (CFGridWriter2)\\nOriginal Dataset = file:/daymet/V3/CFMosaicAgg/na.ncml; Translation Date = 2020-02-11T19:32:13.053Z', 'geospatial_lat_min': 18.552629168112905, 'geospatial_lat_max': 58.1720239604238, 'geospatial_lon_min': -143.08286944629194, 'geospatial_lon_max': -50.72062839852924}\n",
      "\n",
      " The crs meta data is \n",
      " {'grid_mapping_name': 'lambert_conformal_conic', 'longitude_of_central_meridian': -100.0, 'latitude_of_projection_origin': 42.5, 'false_easting': 0.0, 'false_northing': 0.0, 'standard_parallel': array([25., 60.]), 'semi_major_axis': 6378137.0, 'inverse_flattening': 298.257223563, 'longitude_of_prime_meridian': 0.0, '_CoordinateTransformType': 'Projection', '_CoordinateAxisTypes': 'GeoX GeoY'}\n",
      "<xarray.DataArray 'tmax' (time: 1, y: 3699, x: 6008)>\n",
      "[22223592 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * y        (y) float32 1687.0 1686.0 1685.0 1684.0 ... -2009.0 -2010.0 -2011.0\n",
      "  * x        (x) float32 -2754.25 -2753.25 -2752.25 ... 3250.75 3251.75 3252.75\n",
      "  * time     (time) datetime64[ns] 2018-12-31\n",
      "Attributes:\n",
      "    long_name:     daily maximum temperature\n",
      "    units:         degrees C\n",
      "    grid_mapping:  lambert_conformal_conic\n",
      "    cell_methods:  area: mean time: maximum\n",
      "    _ChunkSizes:   [   1 1000 1000]\n",
      "\n",
      " Data attributes, sizes, and coords \n",
      "\n",
      "\n",
      " Data attributes are: \n",
      " {'long_name': 'daily maximum temperature', 'units': 'degrees C', 'grid_mapping': 'lambert_conformal_conic', 'cell_methods': 'area: mean time: maximum', '_ChunkSizes': array([   1, 1000, 1000])}\n",
      "\n",
      " Data sizes are: \n",
      " Frozen({'time': 1, 'y': 3699, 'x': 6008})\n",
      "\n",
      " Data coords are: \n",
      " Coordinates:\n",
      "  * y        (y) float32 1687.0 1686.0 1685.0 1684.0 ... -2009.0 -2010.0 -2011.0\n",
      "  * x        (x) float32 -2754.25 -2753.25 -2752.25 ... 3250.75 3251.75 3252.75\n",
      "  * time     (time) datetime64[ns] 2018-12-31\n",
      "\n",
      " Lat coords are: \n",
      " {'units': 'degrees_north', 'long_name': 'latitude coordinate', 'standard_name': 'latitude', '_ChunkSizes': array([1010,  977]), '_CoordinateAxisType': 'Lat', 'grid_mapping': 'lambert_conformal_conic'}\n",
      "<class 'xarray.core.utils.Frozen'>\n",
      "1\n",
      "1 6008 3699\n"
     ]
    }
   ],
   "source": [
    "#=========================================================\n",
    "#            MACAV2METDATA FILE PARAMETERS\n",
    "#=========================================================\n",
    "# dirPath='https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1328/2018/daymet_v3_prcp_2018_na.nc4?var=lat&var=lon&var=prcp&north=52.880049298000074+&west=-124.72462483099997+&east=-66.94953853699997+&south=24.839424370000074&disableLLSubset=on&disableProjSubset=on&horizStride=1&time_start=2018-12-31T00:00:00Z&time_end=2018-12-31T00:00:00Z&timeStride=1&accept=netcdf'\n",
    "# fileName='/thredds/dodsC/MET/tmmx/tmmx_2019.nc'\n",
    "dirPath = 'tmax_test2.nc'\n",
    "# dirPath = 'daymet_v3_tmax_2018_na.nc4.nc'\n",
    "#--------------------------------------------------------\n",
    "#   FORM FILENAME AND GET HANDLE TO FILE AND DATA\n",
    "#--------------------------------------------------------\n",
    "fullfilename= dirPath\n",
    "print(fullfilename)\n",
    "\n",
    "ds = xr.open_dataset(fullfilename)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# df = ds.to_dataframe()\n",
    "\n",
    "print('\\n The meta data is: \\n', ds.attrs)\n",
    "lathandle=ds['lat']\n",
    "lonhandle=ds['lon']\n",
    "timehandle=ds['time']\n",
    "datahandle=ds['tmax']\n",
    "dhlat = ds['lat']\n",
    "dhlon = ds['lon']\n",
    "crshandle=ds['lambert_conformal_conic']\n",
    "print('\\n The crs meta data is \\n', crshandle.attrs)\n",
    "print(datahandle)\n",
    "# crstransform = crshandle.attrs['GeoTransform']\n",
    "# print(crstransform)\n",
    "\n",
    "#collect data to describe geotransform\n",
    "lonmin = float(ds.attrs['geospatial_lon_min'])\n",
    "latmax = float(ds.attrs['geospatial_lat_max'])\n",
    "# lonres = float(ds.attrs['geospatial_lon_resolution'])\n",
    "# latres = float(ds.attrs['geospatial_lon_resolution'])\n",
    "\n",
    "#Print some information on the data\n",
    "\n",
    "print('\\n Data attributes, sizes, and coords \\n') \n",
    "print('\\n Data attributes are: \\n',datahandle.attrs)\n",
    "print('\\n Data sizes are: \\n', datahandle.sizes)\n",
    "print('\\n Data coords are: \\n', datahandle.coords)\n",
    "print('\\n Lat coords are: \\n', dhlat.attrs)\n",
    "\n",
    "ts = datahandle.sizes\n",
    "print(type(ts))\n",
    "print(ts['time'])\n",
    "dayshape = ts['time']\n",
    "Lonshape = ts['x']\n",
    "Latshape = ts['y']\n",
    "#dayshape,lonshape,latshape = datahandle.values.shape\n",
    "print(dayshape, Lonshape, Latshape)\n",
    "\n",
    "# datahandle.values[dayshape-1,:,:].shape\n",
    "\n",
    "# print(lathandle.values.shape)\n",
    "# print(type(lathandle.values))\n",
    "# print(datahandle.dtype)\n",
    "# print(np.isfortran(datahandle.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.55824\n"
     ]
    }
   ],
   "source": [
    "print(np.min(dhlat.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = ds.variables['tmax'][:]  # Reads the whole array\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tmax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1, ax1 = plt.subplots(1)\n",
    "# ax1.set_aspect('equal')\n",
    "# # ax1.set(xlim=(-130, -60), ylim=(20, 55))\n",
    "# ptmax = ds.tmax\n",
    "# ptmax_1 = ptmax.isel(time=0)\n",
    "# print(type(ptmax_1))\n",
    "# print(ptmax_1.shape)\n",
    "# # ptmax_1.plot(ax=ax1, x='x', y='y', cmap='viridis')\n",
    "# ds.tmax.plot(ax=ax1, x='x', y='y')\n",
    "# f1.tight_layout()\n",
    "# f1.savefig('daymet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3699 6008\n"
     ]
    }
   ],
   "source": [
    "lon = ds.lon.values\n",
    "lat = ds.lat.values\n",
    "print(np.shape(lon)[0], np.shape(lon)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3698"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lon)[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "tindex = []\n",
    "tlat = []\n",
    "tlon = []\n",
    "count = 0\n",
    "for i in range(1, np.shape(lon)[0]-1):\n",
    "    if i%100 == 0: print(i)\n",
    "    for j in range(1, np.shape(lon)[1]-1):\n",
    "        tindex.append(count)\n",
    "        tlat.append(lat[i,j])\n",
    "        tlon.append(lon[i,j])\n",
    "        count+=1\n",
    "        \n",
    "tdf = pd.DataFrame({'index':tindex,\n",
    "                   'lat':tlat,\n",
    "                   'lon':tlon})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>51.521744</td>\n",
       "      <td>-143.052292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51.526348</td>\n",
       "      <td>-143.039291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>51.530949</td>\n",
       "      <td>-143.026276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>51.535549</td>\n",
       "      <td>-143.013275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>51.540150</td>\n",
       "      <td>-143.000259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        lat         lon\n",
       "0      0  51.521744 -143.052292\n",
       "1      1  51.526348 -143.039291\n",
       "2      2  51.530949 -143.026276\n",
       "3      3  51.535549 -143.013275\n",
       "4      4  51.540150 -143.000259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Creating Spatial Index - This could take some time\n",
      "Finished Spatial Index\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon\n",
    "def distance(p1x, p1y, p2x, p2y):\n",
    "    return np.sqrt(np.power((p2x-p1x),2) + np.power((p2y-p1y),2))\n",
    "\n",
    "#first create dataframe with temp\n",
    "# df = pd.DataFrame({'temperature': ds.tmax.values.flatten()})\n",
    "# res = 0.04166666/2.0\n",
    "numcells2 = (np.shape(lat)[0]-2)*(np.shape(lat)[1]-2) # -2 to ignore boundaries, daymet domain should well overlap conus\n",
    "poly2 = []\n",
    "index2 = np.zeros(numcells2)\n",
    "count = 0\n",
    "# ncfcell = gpd.GeoDataFrame()\n",
    "# ncfcell['geometry'] = None\n",
    "\n",
    "for i in range(1, np.shape(lon)[0]-1):\n",
    "    if i%100 == 0: print(i)\n",
    "    for j in range(1, np.shape(lon)[1]-1):\n",
    "        \n",
    "        tpoly_1_lon = [lon[i,j], lon[i,j-1], lon[i+1,j-1], lon[i+1, j]]\n",
    "        tpoly_1_lat = [lat[i,j], lat[i,j-1], lat[i+1,j-1], lat[i+1, j]]\n",
    "        tpoly_1 = Polygon(zip(tpoly_1_lon, tpoly_1_lat))\n",
    "        p1 = tpoly_1.centroid\n",
    "        \n",
    "        tpoly_2_lon = [lon[i,j], lon[i+1,j], lon[i+1,j+1], lon[i, j+1]]\n",
    "        tpoly_2_lat = [lat[i,j], lat[i+1,j], lat[i+1,j+1], lat[i, j+1]]\n",
    "        tpoly_2 = Polygon(zip(tpoly_2_lon, tpoly_2_lat))\n",
    "        p2 = tpoly_2.centroid  \n",
    "        \n",
    "        tpoly_3_lon = [lon[i,j], lon[i,j+1], lon[i-1,j+1], lon[i-1, j]]\n",
    "        tpoly_3_lat = [lat[i,j], lat[i,j+1], lat[i-1,j+1], lat[i-1, j]]\n",
    "        tpoly_3 = Polygon(zip(tpoly_3_lon, tpoly_3_lat))\n",
    "        p3 = tpoly_3.centroid  \n",
    "\n",
    "        tpoly_4_lon = [lon[i,j], lon[i-1,j], lon[i-1,j-1], lon[i, j-1]]\n",
    "        tpoly_4_lat = [lat[i,j], lat[i-1,j], lat[i-1,j-1], lat[i, j-1]]\n",
    "        tpoly_4 = Polygon(zip(tpoly_4_lon, tpoly_4_lat))\n",
    "        p4 = tpoly_4.centroid  \n",
    "        \n",
    "        lon_point_list = [p1.x, p2.x, p3.x, p4.x]\n",
    "        lat_point_list = [p1.y, p2.y, p3.y, p4.y]\n",
    "        \n",
    "        poly2.append(Polygon(zip(lon_point_list, lat_point_list)))\n",
    "        index2[count] = count\n",
    "        count += 1\n",
    "# ncfcells = gpd.GeoDataFrame(df, index=index, crs=ds['crs'], geometry=poly)  \n",
    "ncfcells2 = gpd.GeoDataFrame(tdf, index=index2, geometry=poly2)  \n",
    "ncfcells2.head()\n",
    "print('Creating Spatial Index - This could take some time')\n",
    "spatial_index2 = ncfcells2.sindex\n",
    "print('Finished Spatial Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tcount = 0\n",
    "vhruid = 1217\n",
    "with open('tmp2_weights2t.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for index, row in gdf.iterrows():\n",
    "#         print(index, row)\n",
    "        if np.int(row['hru_id_nat']) == vhruid :\n",
    "            count = 0\n",
    "            if tcount == 0:\n",
    "                writer.writerow(['grid_ids', 'hru_id_nat', 'w'])\n",
    "            possible_matches_index = list(spatial_index2.intersection(row['geometry'].bounds))\n",
    "            if not(len(possible_matches_index) == 0):\n",
    "                possible_matches = ncfcells2.iloc[possible_matches_index]\n",
    "                precise_matches = possible_matches[possible_matches.intersects(row['geometry'])]\n",
    "                if not(len(precise_matches) == 0):\n",
    "                    res_intersection = gpd.overlay(gdf.loc[[index]], precise_matches, how='intersection')\n",
    "                    for nindex, row in res_intersection.iterrows():\n",
    "                        ttt = gdf.loc[[index], 'geometry'].area\n",
    "                        tmpfloat = np.float(res_intersection.area.iloc[nindex]/ttt[index])\n",
    "                        writer.writerow([np.int(precise_matches.index[count]), np.int(row['hru_id_nat']), tmpfloat])\n",
    "                        count += 1\n",
    "                    tcount += 1\n",
    "                    if tcount%100 == 0:\n",
    "                        print(tcount, index)\n",
    "        \n",
    "            else:\n",
    "                print('no intersection: ', index, np.int(row['nhm_id']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tcount = 0\n",
    "vhruid = 1217\n",
    "with open('tmp2_weights2t.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for index, row in gdf.iterrows():\n",
    "#         print(index, row)\n",
    "        if np.int(row['hru_id_nat']) == vhruid :\n",
    "            count = 0\n",
    "            if tcount == 0:\n",
    "                writer.writerow(['grid_ids', 'hru_id_nat', 'w'])\n",
    "            possible_matches_index = list(spatial_index2.intersection(row['geometry'].bounds))\n",
    "            if not(len(possible_matches_index) == 0):\n",
    "                possible_matches = ncfcells2.iloc[possible_matches_index]\n",
    "                precise_matches = possible_matches[possible_matches.intersects(row['geometry'])]\n",
    "                if not(len(precise_matches) == 0):\n",
    "                    res_intersection = gpd.overlay(gdf.loc[[index]], precise_matches, how='intersection')\n",
    "                    for nindex, row in res_intersection.iterrows():\n",
    "                        ttt = gdf.loc[[index], 'geometry'].area\n",
    "                        tmpfloat = np.float(res_intersection.area.iloc[nindex]/ttt[index])\n",
    "                        writer.writerow([np.int(precise_matches.index[count]), np.int(row['hru_id_nat']), tmpfloat])\n",
    "                        count += 1\n",
    "                    tcount += 1\n",
    "                    if tcount%100 == 0:\n",
    "                        print(tcount, index)\n",
    "        \n",
    "            else:\n",
    "                print('no intersection: ', index, np.int(row['nhm_id']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = res_intersection.plot(cmap='tab10')\n",
    "ax = gdf.loc[[index], 'geometry'].plot(ax=ax, facecolor='none', edgecolor='k')\n",
    "precise_matches.plot(ax=ax, facecolor='none', edgecolor='k')\n",
    "# index = 0\n",
    "count = 0\n",
    "cumwght = 0\n",
    "for nindex, row in res_intersection.iterrows():\n",
    "    ttt = gdf.loc[[index], 'geometry'].area\n",
    "    tmpfloat = np.float(res_intersection.area.iloc[nindex]/ttt[index])\n",
    "    # writer.writerow([np.int(precise_matches.index[count]), np.int(row['GFv11_id']), np.int(row['hru_id_nat']), tmpfloat])\n",
    "    #writer.writerow([np.int(precise_matches.index[count]), np.int(row['GFv11_id']), tmpfloat])\n",
    "    cumwght+=tmpfloat\n",
    "    if nindex == 0:\n",
    "        print('daymet_index', 'hru_index', 'intersection_area', 'hru_area', 'weight', 'cumulative_weight')\n",
    "    print(np.int(precise_matches.index[count]), np.int(row['hru_id_nat']), res_intersection.area.iloc[nindex], ttt[index], tmpfloat, cumwght)\n",
    "    count += 1\n",
    "print(res_intersection.area, np.sum(res_intersection.area), gdf.loc[[index], 'geometry'].area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tcount = 0\n",
    "with open('tmp2_weights2t.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for index, row in gdf.iterrows():\n",
    "        count = 0\n",
    "        if tcount == 0:\n",
    "            writer.writerow(['grid_ids', 'hru_id_nat', 'w'])\n",
    "        possible_matches_index = list(spatial_index2.intersection(row['geometry'].bounds))\n",
    "        if not(len(possible_matches_index) == 0):\n",
    "            possible_matches = ncfcells2.iloc[possible_matches_index]\n",
    "            precise_matches = possible_matches[possible_matches.intersects(row['geometry'])]\n",
    "            if not(len(precise_matches) == 0):\n",
    "                res_intersection = gpd.overlay(gdf.loc[[index]], precise_matches, how='intersection')\n",
    "                for nindex, row in res_intersection.iterrows():\n",
    "                    ttt = gdf.loc[[index], 'geometry'].area\n",
    "                    tmpfloat = np.float(res_intersection.area.iloc[nindex]/ttt[index])\n",
    "                    writer.writerow([np.int(precise_matches.index[count]), np.int(row['hru_id_nat']), tmpfloat])\n",
    "                    count += 1\n",
    "                tcount += 1\n",
    "                if tcount%100 == 0:\n",
    "                    print(tcount, index)\n",
    "        \n",
    "            else:\n",
    "                print('no intersection: ', index, np.int(row['nhm_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.ma import masked\n",
    "# add tmax column to dataframe\n",
    "gdf['tmax']=0.0\n",
    "# gdf.tmax[noint] = 10.0\n",
    "# print(gdf.tmax[noint])\n",
    "\n",
    "wght_UofI = pd.read_csv('tmp2_weights2t.csv')\n",
    "print(wght_UofI.head())\n",
    "\n",
    "#iterate through hru's, grab all weights associated with hru_id, get total weighted value from netcdf file, assign to tmax\n",
    "# ndata = datahandle.values[dayshape-1,:,:].flatten(order='C')\n",
    "# ndata = ds.tmax.values[dayshape-1,:,:].flatten()\n",
    "lon = ds.lon.values\n",
    "lat = ds.lat.values\n",
    "ndata = np.zeros(np.shape(lon)[1]*np.shape(lon)[0])\n",
    "tlc = 0\n",
    "for i in range(1, np.shape(lon)[0]-1):\n",
    "    if i%10 == 0: print(i)\n",
    "    for j in range(1, np.shape(lon)[1]-1):\n",
    "        ndata[tlc] = datahandle.values[dayshape-1,i,j]\n",
    "        tlc+=1\n",
    "\n",
    "# print(ndata[1000:])\n",
    "hruid = 'GFv11_id'\n",
    "hruid = 'hru_id_nat'\n",
    "unique_hru_ids = wght_UofI.groupby(hruid)\n",
    "print(len(gdf), len(unique_hru_ids))\n",
    "\n",
    "def np_get_wval2(grp, ndata):\n",
    "    mdata = np.ma.masked_array(ndata[grp['grid_ids'].values.astype(int)], np.isnan(ndata[grp['grid_ids'].values.astype(int)]))\n",
    "    return np.ma.average(mdata, weights=grp['w'])\n",
    "def np_get_wval(ndata, wghts, hru_id):\n",
    "    \"\"\"\n",
    "    Returns weighted average of ndata with weights = grp\n",
    "    1) mdata = the subset of values associated with the gridmet id's that are mapped to hru_id.\n",
    "    2) Some of these values may have nans if the gridmet id is outside of conus so only return values\n",
    "    that are inside of conus\n",
    "    3) this means that hru's that are entirely outside of conus will return nans which will ultimately,\n",
    "    outside of this function get assigned zero's.\n",
    "    4) the value is assigned the weighted average\n",
    "    :param ndata: float array of data values\n",
    "    :param wghts: float array of weights\n",
    "    :param hru_id hru id number\n",
    "    :return: numpy weighted averaged - masked to deal with nans associated with\n",
    "            ndata that is outside of the conus.\n",
    "    \"\"\"\n",
    "    mdata = np.ma.masked_array(ndata[wghts['grid_ids'].values.astype(int)],\n",
    "                               np.isnan(ndata[wghts['grid_ids'].values.astype(int)]))\n",
    "\n",
    "    # mdata = np.ma.masked_where(ndata[wghts['grid_ids'].values.astype(int)] <= 0.0,\n",
    "    #                            (ndata[wghts['grid_ids'].values.astype(int)]))\n",
    "    tmp = np.ma.average(mdata, weights=wghts['w'])\n",
    "    if tmp is masked:\n",
    "        # print('returning masked value', hru_id, mdata, wghts['w'])\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        return tmp\n",
    "    \n",
    "# unique_hru_ids.get_group(gdf['hru_id_nat']).agg({'tmax': np_get_wval(weight_id_rows, ndata)})    \n",
    "td = np.zeros(len(gdf.index))\n",
    "for index, row in gdf.iterrows():\n",
    "#     if not(row['nhm_id'] in noint):\n",
    "    try: \n",
    "        weight_id_rows = unique_hru_ids.get_group(row[hruid])\n",
    "        td[index] = np.nan_to_num(np_get_wval(ndata, weight_id_rows, index+1))\n",
    "    except KeyError:\n",
    "        td[index] = 0.0\n",
    "            \n",
    "            \n",
    "#         gdf['tmax'][index] = np_get_wval2(weight_id_rows, ndata)\n",
    "#     else:\n",
    "#         td[index] = 10.0\n",
    "\n",
    "gdf['tmax'] = td.tolist()\n",
    "# gdf['tmax'].fillna(0.0)\n",
    "\n",
    "print('min/max', gdf['tmax'].min(), gdf['tmax'].max())\n",
    "\n",
    "f, ax = plt.subplots(2, figsize=(12,12))\n",
    "gdf.plot(ax=ax[0], column = 'tmax',linewidth=0., edgecolor='white')\n",
    "ax[0].set_aspect('equal')\n",
    "ptmax = ds.tmax\n",
    "ptmax_1 = ptmax.isel(time=dayshape-1)\n",
    "lvs = np.arange(gdf['tmax'].min(), gdf['tmax'].max(), 0.5)\n",
    "ptmax_1.plot(ax=ax[1], levels=lvs, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, figsize=(12,12))\n",
    "gdf.plot(ax=ax[0], column = 'tmax',linewidth=0., edgecolor='white')\n",
    "ptmax = ds.tmax\n",
    "ptmax_1 = ptmax.isel(time=dayshape-1)\n",
    "lvs = np.arange(gdf['tmax'].min(), gdf['tmax'].max(), 0.5)\n",
    "ptmax_1.plot(ax=ax[1], levels=lvs, cmap='viridis')\n",
    "# print(temp)\n",
    "# print(temp.lat)\n",
    "hru_1 = ptmax_1.where((ds.lon>=-74) & (ds.lon<=-67) & (ds.lat>=40.5) & (ds.lat<=48.5), drop=True)\n",
    "# print(delaware)\n",
    "lvs = np.arange(gdf['tmax'].min(), gdf['tmax'].max(), 0.5)\n",
    "p=hru_1.plot(ax=ax[1], levels=lvs, cmap='viridis')\n",
    "ax[1].set_aspect('equal','box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
